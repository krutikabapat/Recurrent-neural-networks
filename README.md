# Recurrent-neural-networks
This project mainly focuses on generating echoes of a given sequence using RNNs.



The basic Idea behind RNN is it gives same value of Weights and Bias to all the hidden Layers and maintains a memory to remember the last input as well.
The input to the RNN at every time-step is the current value as well as a state vector which represent what the network has “seen” at time-steps before. This state-vector is the encoded memory of the RNN, initially set to zero.

## Usage:-

<code> python RNN.py </code>    

## References:    

1. https://medium.com/@purnasaigudikandula/recurrent-neural-networks-and-lstm-explained-7f51c7f6bbb9    
2. https://medium.com/mindorks/understanding-the-recurrent-neural-network-44d593f112a2    



